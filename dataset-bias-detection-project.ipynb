{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFeYS8oNctrw"
      },
      "source": [
        "# Milestone 1: Download and prepare the RedditBias data into train/test/val splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npWWIxZJAUM2"
      },
      "source": [
        "Task 1 - Load the repo in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4ZlebCPuyN7"
      },
      "outputs": [],
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/umanlp/RedditBias.git\n",
        "\n",
        "# Installing necessary packages\n",
        "!pip install --upgrade pip\n",
        "!pip install numpy pandas scikit-learn\n",
        "\n",
        "# Go inside the RedditBias Directory\n",
        "%cd RedditBias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnPVyPt5Ahd6"
      },
      "source": [
        "Task 2 - Combine annotated csv files to formulate 1 final dataset (shuffle the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhXp_DQBj1zI"
      },
      "outputs": [],
      "source": [
        "#add as needed\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oARysh-zYSBt"
      },
      "outputs": [],
      "source": [
        "files = [\n",
        "    \"data/gender/reddit_comments_gender_female_processed_phrase_annotated.csv\",\n",
        "    \"data/orientation/reddit_comments_orientation_lgbtq_processed_phrase_annotated.csv\",\n",
        "    \"data/race/reddit_comments_race_black_processed_phrase_annotated.csv\",\n",
        "    \"data/religion1/reddit_comments_religion1_jews_processed_phrase_annotated.csv\",\n",
        "    \"data/religion2/reddit_comments_religion2_muslims_processed_phrase_annotated.csv\"\n",
        "]\n",
        "\n",
        "# Load and combine annotated data\n",
        "annotated = [pd.read_csv(file, encoding=\"cp1252\") for file in files]\n",
        "df = pd.concat(annotated, ignore_index=True)\n",
        "\n",
        "# Shuffle\n",
        "df = df.sample(frac=1, random_state=1234).reset_index(drop=True)\n",
        "\n",
        "# Drop extra columns\n",
        "df = df.drop(columns=[\"Unnamed: 5\", \"id\"])\n",
        "\n",
        "# Preview\n",
        "print(df.shape)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiqjbxGgcr_4"
      },
      "source": [
        "Task 3 - Data Cleaning and NLP Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V6lg81L8O2G"
      },
      "outputs": [],
      "source": [
        "# data preparation\n",
        "# examine column names and types\n",
        "print(df.info())\n",
        "print(df.columns)\n",
        "\n",
        "# overview\n",
        "print(df.sample(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1bEbvAB9OWE"
      },
      "outputs": [],
      "source": [
        "# count of null values per column\n",
        "print('null check:')\n",
        "print(df.isnull().sum())\n",
        "print()\n",
        "\n",
        "# drop rows where 'bias_sent' label is null (for sentence-level task)\n",
        "# can drop 'bias_phrase' for phrase-level tasks\n",
        "df_clean = df.dropna(subset=['bias_sent'])\n",
        "\n",
        "print('null check after dropping:')\n",
        "print(df_clean.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svmdvjKSEcOj"
      },
      "outputs": [],
      "source": [
        "# check for duplicates\n",
        "print('duplicate sum:')\n",
        "print(df.duplicated().sum)\n",
        "print()\n",
        "\n",
        "# view duplciated rows\n",
        "print('duplicate rows:')\n",
        "print(df[df.duplicated()])\n",
        "print()\n",
        "\n",
        "# drop duplicate rows\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "\n",
        "print('duplicate sum after dropping duplicates:')\n",
        "print(df_clean.duplicated().sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjGzR8tFIhMd"
      },
      "outputs": [],
      "source": [
        "# Examine the final cleaned dataset state and plan next NLP preprocessing steps\n",
        "print(f\"Final dataset shape after cleaning: {df_clean.shape}\")\n",
        "print(f\"Label distribution in bias_sent:\")\n",
        "print(df_clean['bias_sent'].value_counts())\n",
        "print(\"\\nSample of cleaned text data:\")\n",
        "print(df_clean['comment'].head(3).tolist())\n",
        "print(\"\\nText characteristics to address:\")\n",
        "print(f\"Average comment length: {df_clean['comment'].str.len().mean():.1f} chars\")\n",
        "print(f\"Comments with special chars: {df_clean['comment'].str.contains('[^a-zA-Z0-9\\\\s]').sum()}\")\n",
        "print(f\"Comments with URLs: {df_clean['comment'].str.contains('http|www').sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZpDYwDONHFy"
      },
      "outputs": [],
      "source": [
        "# text normalization\n",
        "import re\n",
        "\n",
        "def normalize_text(text: str):\n",
        "  # convert to string first if NaN or non-string\n",
        "  if not isinstance(text, str):\n",
        "    return \"\"\n",
        "\n",
        "  # lowercase\n",
        "  text = text.lower()\n",
        "  # remove urls\n",
        "  text = re.sub(r\"http[s]?://\\S+\", \"\", text)  # identifies and removes url by identifying http(s) until space\n",
        "  text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
        "  # remove mentions/user handles (might not be necessary)\n",
        "  text = re.sub(r\"@\\w+\", \"\", text)\n",
        "  # expand simple contractions (can expand this dict as needed)\n",
        "  contractions = {\"can't\": \"cannot\", \"won't\": \"will not\", \"don't\": \"do not\", \"i'm\": \"i am\"}\n",
        "  for c, full in contractions.items():\n",
        "      text = text.replace(c, full)\n",
        "  # remove special characters (but keep basic punctuation)\n",
        "  text = re.sub(r\"[^a-z0-9\\s.,!?'-]\", \"\", text)\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "  return text\n",
        "\n",
        "# normalizes all comments into a new column comment_norm\n",
        "df_clean[\"comment_norm\"] = df_clean[\"comment\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu12VreONJzG"
      },
      "outputs": [],
      "source": [
        "#Normalizing and dropping unwanted values in df_clean\n",
        "\n",
        "#Exploring df_clean\n",
        "df_clean.head(10)\n",
        "\n",
        "print(\"Values in bias_sent:\")\n",
        "print(df_clean[\"bias_sent\"].unique())\n",
        "\n",
        "print(\"\\nValues in bias_phrase:\")\n",
        "print(df_clean[\"bias_phrase\"].unique()) #did not clean as we just focused on bias_sent for now\n",
        "\n",
        "print(f\"\\nNumber of unique values in bias_sent:\\n{df_clean['bias_sent'].nunique(dropna = False)}\\n\")\n",
        "print(f\"Value Count: \\n{df_clean['bias_sent'].value_counts(dropna = False)}\")\n",
        "\n",
        "#Dropping the \"biased?\", \"re-state\", \"question\", \"fact?\", \"1 - context needed\", and \"toxic-unrelated\" columns; make up small amount of data\n",
        "\n",
        "unwanted_values = [\"biased?\", \"re-state\", \"question\", \"fact?\", \"1 - context needed\", \"toxic-unrelated\"]\n",
        "\n",
        "to_drop = df_clean[df_clean['bias_sent'].isin(unwanted_values)].index\n",
        "df_clean = df_clean.drop(to_drop)\n",
        "\n",
        "print(f\"\\nNew Value Count: \\n{df_clean['bias_sent'].value_counts(dropna=False)}\")\n",
        "\n",
        "print(f\"Type: {df_clean['bias_sent'].dtype}\")\n",
        "\n",
        "#Normalizing floats and strings to int\n",
        "\n",
        "df_clean['bias_sent'] = df_clean['bias_sent'].replace({'0':0,'1':1, '2':2, 0.0: 0, 1.0:1, 2.0:2})\n",
        "df_clean['bias_sent'] = df_clean['bias_sent'].astype(int)\n",
        "\n",
        "print(f\"\\nType: {df_clean['bias_sent'].dtype}\")\n",
        "\n",
        "#Dropping rows with value of 2\n",
        "\n",
        "df_clean = df_clean[df_clean['bias_sent']!= 2]\n",
        "\n",
        "print(f\"\\nNew Value Count: \\n{df_clean['bias_sent'].value_counts(dropna=False)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaZVCpkGO-3n"
      },
      "source": [
        "Task 4 - Implement feature engineering techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_pCVvSyURGa"
      },
      "outputs": [],
      "source": [
        "# add feature engineering columns for features like word counts, uppercase ratio, and trigger word presence\n",
        "\n",
        "# adding word count to each row\n",
        "df_clean[\"word_count\"] = df_clean[\"comment\"].apply(lambda comment: len(comment.split()))\n",
        "\n",
        "# uppercase ratio: uppercase letters / total letters\n",
        "def calculate_uppercase_ratio(comment: str) -> float:\n",
        "  total_count = len(comment)\n",
        "  uppercase_count = sum(1 for letter in comment if letter.isupper())\n",
        "  return uppercase_count / total_count\n",
        "\n",
        "df_clean[\"uppercase_ratio\"] = df[\"comment\"].apply(lambda comment: calculate_uppercase_ratio(comment))\n",
        "\n",
        "print('uppercase ratio')\n",
        "print(df_clean[\"uppercase_ratio\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVCSdwE-aq1_"
      },
      "outputs": [],
      "source": [
        "# load trigger words\n",
        "# note: a lot have asterisks to indicate a root / stem\n",
        "# better to separate into positive and negative trigger files\n",
        "trigger_word_files = [\n",
        "  # gender trigger words\n",
        "  \"data/gender/gender_female.txt\",\n",
        "  \"data/gender/gender_female_attributes.txt\",\n",
        "  \"data/gender/gender_female_extra.txt\",\n",
        "  \"data/gender/gender_female_old.txt\",\n",
        "  \"data/gender/gender_female_pos.txt\",\n",
        "  \"data/gender/gender_opposites.txt\", # phrases\n",
        "  # orientation trigger words\n",
        "  \"data/orientation/orientation_lgbtq.txt\",\n",
        "  \"data/orientation/orientation_lgbtq_not_used_annotation.txt\",\n",
        "  \"data/orientation/orientation_lgbtq_pos.txt\",\n",
        "  \"data/orientation/orientation_opposites.txt\", # phrases\n",
        "  # race trigger words\n",
        "  \"data/race/black_words.txt\",\n",
        "  \"data/race/race_black.txt\",\n",
        "  \"data/race/race_black_pos.txt\",\n",
        "  \"data/race/race_opposites.txt\", # phrases\n",
        "  # religion trigger words\n",
        "  \"data/religion1/religion1_jews.txt\",\n",
        "  \"data/religion1/religion1_jews_pos.txt\",\n",
        "  \"data/religion1/religion1_opposites.txt\",\n",
        "  \"data/religion2/religion2_muslims.txt\",\n",
        "  \"data/religion2/religion2_muslims_pos.txt\",\n",
        "  \"data/religion2/religion2_opposites.txt\" # phrases\n",
        "]\n",
        "\n",
        "negative_trigger_files = []\n",
        "positive_trigger_files = []\n",
        "phrase_opposites_files = []\n",
        "# phrase_opposites files don't add much value\n",
        "\n",
        "for path in trigger_word_files:\n",
        "  filename = path.split('/')[-1]\n",
        "  if re.search(r'_pos\\.txt$', filename):\n",
        "    positive_trigger_files.append(path)\n",
        "  elif re.search(r'_opposites\\.txt$', filename):\n",
        "    phrase_opposites_files.append(path)\n",
        "  else:\n",
        "    negative_trigger_files.append(path)\n",
        "\n",
        "def load_trigger_set(file_paths):\n",
        "    trigger_set = set() # set bc its whole words where exact matches can easily be found\n",
        "    stem_triggers = []\n",
        "    phrase_triggers = []\n",
        "\n",
        "    for path in file_paths:\n",
        "        with open(path, 'r', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#'):  # skip blanks/comments\n",
        "                    continue\n",
        "                # handle phrase pairs: if comma, use each side\n",
        "                if ',' in line:\n",
        "                    parts = [p.replace('\"', '').replace('*', '').lower().strip() for p in line.split(',')]\n",
        "                    parts = [p for p in parts if p != \"\"]\n",
        "                    phrase_triggers.extend(parts)\n",
        "                # Handle asterisks (wildcard/stem triggers)\n",
        "                elif '*' in line:\n",
        "                    stem = line.replace('*', '').replace('\"', '').lower().strip()\n",
        "                    if stem != \"\":\n",
        "                      stem_triggers.append(stem)\n",
        "                # Else add as literal word/phrase\n",
        "                else:\n",
        "                    trigger = line.replace('\"', '').lower().strip()\n",
        "                    if trigger != \"\":\n",
        "                      trigger_set.add(trigger)\n",
        "\n",
        "    return trigger_set, stem_triggers, phrase_triggers\n",
        "\n",
        "neg_trigger_set, neg_stem_triggers, neg_phrase_triggers = load_trigger_set(negative_trigger_files)\n",
        "\n",
        "print(\"negative trigger set\")\n",
        "print(neg_trigger_set)\n",
        "\n",
        "print(\"neg_stem triggers\")\n",
        "print(neg_stem_triggers)\n",
        "\n",
        "print(\"neg_phrase_triggers\")\n",
        "print(neg_phrase_triggers)\n",
        "\n",
        "\n",
        "pos_trigger_set, pos_stem_triggers, pos_phrase_triggers = load_trigger_set(positive_trigger_files)\n",
        "\n",
        "print(\"positive trigger set\")\n",
        "print(pos_trigger_set)\n",
        "\n",
        "print(\"pos_stem triggers\")\n",
        "print(neg_stem_triggers)\n",
        "\n",
        "print(\"pos_phrase_triggers\")\n",
        "print(pos_phrase_triggers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnmI46MJhWdh"
      },
      "outputs": [],
      "source": [
        "# trigger word features\n",
        "\n",
        "# def has_trigger(comment, trigger_set, stem_triggers, phrase_triggers):\n",
        "#     for phrase in phrase_triggers:\n",
        "#         if phrase.lower() in comment:\n",
        "#             return True\n",
        "#     for word in trigger_set:\n",
        "#         if re.search(r'\\b' + re.escape(word.lower()) + r'\\b', comment):\n",
        "#             return True\n",
        "#     for stem in stem_triggers:\n",
        "#         if re.search(r'\\b' + re.escape(stem.lower()), comment):\n",
        "#             return True\n",
        "#     return False\n",
        "\n",
        "def extract_trigger_features(comment, negative_set, positive_set, negative_stems, positive_stems, phrases_neg, phrases_pos):\n",
        "    text = comment.lower()\n",
        "    has_neg = any(phrase in text for phrase in phrases_neg) or \\\n",
        "              any(re.search(r'\\b' + re.escape(w), text) for w in negative_stems) or \\\n",
        "              any(re.search(r'\\b' + re.escape(w) + r'\\b', text) for w in negative_set)\n",
        "\n",
        "    has_pos = any(phrase in text for phrase in phrases_pos) or \\\n",
        "              any(re.search(r'\\b' + re.escape(w), text) for w in positive_stems) or \\\n",
        "              any(re.search(r'\\b' + re.escape(w) + r'\\b', text) for w in positive_set)\n",
        "\n",
        "    return has_neg, has_pos\n",
        "\n",
        "df_clean[[\"has_neg_trigger\", \"has_pos_trigger\"]] = df_clean[\"comment\"].apply(\n",
        "    lambda comment: extract_trigger_features(\n",
        "        comment, neg_trigger_set, pos_trigger_set, neg_stem_triggers, pos_stem_triggers, neg_phrase_triggers, pos_phrase_triggers\n",
        "    )\n",
        ").apply(pd.Series)\n",
        "\n",
        "print(df_clean[['comment_norm', 'has_neg_trigger', 'has_pos_trigger']].head())\n",
        "print(df_clean['has_neg_trigger'].value_counts())\n",
        "print(df_clean['has_pos_trigger'].value_counts())\n",
        "\n",
        "print('has negative trigger sampling')\n",
        "print(df_clean[df_clean['has_neg_trigger'] == False][['comment_norm']].head())\n",
        "\n",
        "print('has positive trigger sampling')\n",
        "print(df_clean[df_clean['has_pos_trigger'] == True][['comment_norm']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNZ8QTZwyxgu"
      },
      "outputs": [],
      "source": [
        "df_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH9nHP5KCatG"
      },
      "source": [
        "Task 5 - Split dataset into training/validation/test sets with a fixed random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0Ac0-D3PEZL"
      },
      "outputs": [],
      "source": [
        "#tokenize the phrase\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_task = df_clean.copy()\n",
        "df_task[\"text\"] = df_task[\"phrase\"].fillna(df_task[\"comment\"]).map(normalize_text).astype(str)\n",
        "df_task = df_task[[\"text\", \"bias_sent\"]].dropna()\n",
        "\n",
        "X = df_task[\"text\"].values\n",
        "y = df_task[\"bias_sent\"].values\n",
        "\n",
        "# Stratified split (train/val/test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaxGF0KwtlOR"
      },
      "outputs": [],
      "source": [
        "df_task.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6plOFXOCjo5"
      },
      "source": [
        "Task 6 - Conduct exploratory data analysis to generate summary statistics and key predictive features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z1WX_7SQ9yQ"
      },
      "outputs": [],
      "source": [
        "#tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#CountVectorizer = tokenization + vocab + raw token counts as features\n",
        "bow_clf = Pipeline([\n",
        "    (\"bow\", CountVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9)),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "bow_clf.fit(X_train, y_train)\n",
        "print(\"BOW → Val report\")\n",
        "print(classification_report(y_val, bow_clf.predict(X_val)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd-JUI0LSXBL"
      },
      "outputs": [],
      "source": [
        "#vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#final vectorizer + model (best config)\n",
        "final_clf = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        ngram_range=(1,2),   #unigrams + bigrams\n",
        "        sublinear_tf=True,   #log scaling\n",
        "        min_df=3,            #drop rare tokens\n",
        "        max_df=0.95          #drop overly common tokens\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "import numpy as np\n",
        "X_trval = np.concatenate([X_train, X_val])\n",
        "y_trval = np.concatenate([y_train, y_val])\n",
        "\n",
        "final_clf.fit(X_trval, y_trval)\n",
        "\n",
        "print(\"Final Model → TEST report\")\n",
        "print(classification_report(y_test, final_clf.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20wtnW0stGCk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Label distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=\"bias_sent\", data=df_clean, palette=\"coolwarm\")\n",
        "plt.title(\"Label Distribution (0 = Non-Biased, 1 = Biased)\")\n",
        "plt.xlabel(\"Bias Sent Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Word count distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(data=df_clean, x=\"word_count\", hue=\"bias_sent\", bins=40,\n",
        "             kde=False, palette=\"coolwarm\", alpha=0.6)\n",
        "plt.title(\"Word Count Distribution by Label\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlim(0,100)  # adjust if you want to focus on shorter comments\n",
        "plt.show()\n",
        "\n",
        "# Descriptive stats\n",
        "print(\"Descriptive statistics (word_count by label):\")\n",
        "print(df_clean.groupby(\"bias_sent\")[\"word_count\"].describe())\n",
        "\n",
        "# Trigger word correlation with label\n",
        "print(\"Crosstab: Negative Trigger vs Label\")\n",
        "print(pd.crosstab(df_clean[\"has_neg_trigger\"], df_clean[\"bias_sent\"],\n",
        "                  normalize=\"columns\"))\n",
        "\n",
        "print(\"\\nCrosstab: Positive Trigger vs Label\")\n",
        "print(pd.crosstab(df_clean[\"has_pos_trigger\"], df_clean[\"bias_sent\"],\n",
        "                  normalize=\"columns\"))\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=5, max_df=0.95)\n",
        "X_counts = vectorizer.fit_transform(df_task[\"text\"])\n",
        "y = df_task[\"bias_sent\"]\n",
        "\n",
        "chi2_scores, p_values = chi2(X_counts, y)\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Get top features for biased and non-biased\n",
        "top_biased = feature_names[np.argsort(chi2_scores)[-20:]]\n",
        "top_nonbiased = feature_names[np.argsort(chi2_scores)[:20]]\n",
        "\n",
        "print(\"Top Chi2 Features (biased):\", top_biased)\n",
        "print(\"Top Chi2 Features (non-biased):\", top_nonbiased)\n",
        "\n",
        "# Fit logistic regression with TF-IDF again to inspect coefficients\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), sublinear_tf=True,\n",
        "                             min_df=3, max_df=0.95)\n",
        "X_tfidf = vectorizer.fit_transform(df_task[\"text\"])\n",
        "clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "clf.fit(X_tfidf, y)\n",
        "\n",
        "# Extract feature importance\n",
        "coef = clf.coef_[0]\n",
        "top_positive_idx = np.argsort(coef)[-20:]\n",
        "top_negative_idx = np.argsort(coef)[:20]\n",
        "\n",
        "print(\"Top Predictive Features (Biased):\")\n",
        "for i in top_positive_idx:\n",
        "    print(f\"{vectorizer.get_feature_names_out()[i]} ({coef[i]:.3f})\")\n",
        "\n",
        "print(\"\\nTop Predictive Features (Non-Biased):\")\n",
        "for i in top_negative_idx:\n",
        "    print(f\"{vectorizer.get_feature_names_out()[i]} ({coef[i]:.3f})\")\n",
        "\n",
        "# Numerical features\n",
        "feature_cols = [\"word_count\", \"uppercase_ratio\", \"has_neg_trigger\", \"has_pos_trigger\", \"bias_sent\"]\n",
        "corr = df_clean[feature_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KqDVJG8cK4M"
      },
      "source": [
        "# Milestone 2: Finetune the base model using the RedditBias dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci_E3zNhqmMx"
      },
      "source": [
        "Task 1 - Base model & environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz5_9YMZqm5s"
      },
      "outputs": [],
      "source": [
        "#switch to GPU\n",
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU type:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w_HXHS4ErIT_"
      },
      "outputs": [],
      "source": [
        "#install HF tools\n",
        "!pip install transformers datasets evaluate accelerate scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qDedRJrrN9L"
      },
      "outputs": [],
      "source": [
        "#use cased or uncased -> uncased is ok\n",
        "import re\n",
        "\n",
        "#check how many comments contain uppercase\n",
        "has_upper = df_clean[\"comment\"].str.contains(r\"[A-Z]\").sum()\n",
        "total = len(df_clean)\n",
        "print(f\"{has_upper}/{total} comments contain uppercase ({has_upper/total:.1%})\")\n",
        "\n",
        "#show a few examples where uppercase might matter\n",
        "examples = df_clean[df_clean[\"comment\"].str.contains(r\"[A-Z]\")][\"comment\"].head(10).tolist()\n",
        "for ex in examples:\n",
        "    print(\"•\", ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQSxRILKrsLu"
      },
      "outputs": [],
      "source": [
        "#load tokenizer & model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"  # chosen based on casing check\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    id2label={0: \"non_biased\", 1: \"biased\"},\n",
        "    label2id={\"non_biased\": 0, \"biased\": 1},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbttABtTf-DD"
      },
      "source": [
        "Task 2 - Preprocessing pipeline and data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxdBPi4ZsNzh"
      },
      "outputs": [],
      "source": [
        "#wrap existing splits to HF\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
        "val_dataset   = Dataset.from_dict({\"text\": X_val,   \"label\": y_val})\n",
        "test_dataset  = Dataset.from_dict({\"text\": X_test,  \"label\": y_test})\n",
        "\n",
        "ds = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset, \"test\": test_dataset})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo8u6VMmsoQu"
      },
      "outputs": [],
      "source": [
        "#tokenize (128 to start)\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
        "\n",
        "ds_tok = ds.map(tokenize_batch, batched=True) #tokenizes training, test, and val sets\n",
        "ds_tok = ds_tok.remove_columns([\"text\"])  # keep input_ids, attention_mask, label\n",
        "\n",
        "print(f'\\nExample of token:\\nEncoded: {ds_tok[\"train\"][0][\"input_ids\"]}\\n')\n",
        "print(f'Decoded: {tokenizer.decode(ds_tok[\"train\"][0][\"input_ids\"])}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JozQ8xghelpR"
      },
      "source": [
        "(cont.) Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBJ_6aY6bA3o"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# Step 1: Initialize Data Collator\n",
        "# DataCollatorWithPadding handles padding dynamically for each batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Step 2: Set Batch Size\n",
        "# Batch size = how many examples the model processes at once\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Step 3: Create DataLoaders for Each Split\n",
        "# Training loader: randomize order each epoch\n",
        "# Shuffling prevents the model from learning patterns based on data order\n",
        "train_loader = DataLoader(\n",
        "    ds_tok[\"train\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,                    # Shuffle training data\n",
        "    collate_fn=data_collator         # Use dynamic padding\n",
        ")\n",
        "\n",
        "# Validation loader for consistent evaluation\n",
        "# Don't shuffle validation data so results are reproducible\n",
        "val_loader = DataLoader(\n",
        "    ds_tok[\"validation\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,                   # Don't shuffle validation\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "# Test loader for consistent evaluation\n",
        "test_loader = DataLoader(\n",
        "    ds_tok[\"test\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,                   # Don't shuffle test\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "# Step 4: Verify Everything Works\n",
        "print(\"Data Loaders successfully created\")\n",
        "\n",
        "# Show how many batches per split\n",
        "print(f\"\\nBatch Information:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE} examples per batch\")\n",
        "print(f\"  Training batches: {len(train_loader)} batches\")\n",
        "print(f\"  Validation batches: {len(val_loader)} batches\")\n",
        "print(f\"  Test batches: {len(test_loader)} batches\")\n",
        "\n",
        "# Show total examples\n",
        "print(f\"\\nDataset Sizes:\")\n",
        "print(f\"  Training examples: {len(ds_tok['train'])} samples\")\n",
        "print(f\"  Validation examples: {len(ds_tok['validation'])} samples\")\n",
        "print(f\"  Test examples: {len(ds_tok['test'])} samples\")\n",
        "\n",
        "# Step 5: Verify Loaders Work with Sample Batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "\n",
        "print(f\"\\nSample Batch Structure:\")\n",
        "print(f\"  Keys: {list(sample_batch.keys())}\")\n",
        "print(f\"  input_ids shape: {sample_batch['input_ids'].shape}\")\n",
        "print(f\"  attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
        "print(f\"  labels shape: {sample_batch['labels'].shape}\")\n",
        "\n",
        "print(f\"\\nData loaders ready for training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kkw7U6vilXF"
      },
      "source": [
        "Task 3 - Finetune with default hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v73jbV3xqAbi"
      },
      "outputs": [],
      "source": [
        "# Add WandB to track experiments (hyperparameters)\n",
        "!pip install wandb\n",
        "!wandb login\n",
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13rSrlrkmSUN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# (modified -> use for tracking) os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import evaluate\n",
        "import time\n",
        "import wandb\n",
        "wandb.finish()\n",
        "\n",
        "# try with linear with different learning rate (1e^-4)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"test-trainer\",\n",
        "    report_to = \"wandb\",\n",
        "    run_name = \"run_\" + str(time.time()),\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    per_device_eval_batch_size = BATCH_SIZE,\n",
        "    learning_rate = 2.5e-4,\n",
        "    lr_scheduler_type=\"linear\", # want to adapt\n",
        "    warmup_ratio=0.05, # stabilize early updates\n",
        "    num_train_epochs=5,\n",
        "    weight_decay = 0.01,\n",
        "    max_grad_norm=0.8,\n",
        "    eval_strategy=\"epoch\", # can change to steps if we want to evaluate every X training steps instead of once per full epoch\n",
        "    # eval_steps=100,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy= \"epoch\", # Log metric at every epoch\n",
        "    save_strategy= \"epoch\", # Save model checkpoint in 'output_dir' at every epoch\n",
        "    save_total_limit = 2, # Save last two checkpoint\n",
        "    # use the same training data\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = \"eval_loss\",\n",
        "    greater_is_better = False,\n",
        "    seed=42,\n",
        "    fp16=False, # mixed precision on T4 for smoother training\n",
        "    gradient_accumulation_steps=2 # smaller effective step size\n",
        ")\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred): #need to make this function to pass into parameter compute_metrics to get our metrics! - passes in a function that accepts eval_pred (tuple of logits & labels) and returns a dictionary of the metric names and float vals\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis = -1) #converts logits to predicted class, these two lines are standard for this function\n",
        "\n",
        "  f1 = f1_metric.compute(predictions = predictions, references = labels)\n",
        "  precision = precision_metric.compute(predictions = predictions, references = labels)\n",
        "  recall = recall_metric.compute(predictions = predictions, references = labels) #compute returns a dictionary\n",
        "  accuracy_res = accuracy.compute(predictions = predictions, references = labels)\n",
        "\n",
        "  return {\"accuracy\": accuracy_res[\"accuracy\"], \"f1\" : f1[\"f1\"], \"precision\" : precision[\"precision\"], \"recall\": recall[\"recall\"]}\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,    #Trainer will build the data loaders using data collator\n",
        "    train_dataset = ds_tok[\"train\"],  #passing in tokenized datasets\n",
        "    eval_dataset = ds_tok[\"validation\"],\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics, #Easy way for us to get our metrics after each eval phase\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyDrQxbfDmBk"
      },
      "source": [
        "Task 4 - Evaluate the model performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fVEyU5X6ao-"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating base (untrained) model:\")\n",
        "eval_base_model = trainer.evaluate()\n",
        "for metric, value in eval_base_model.items():\n",
        "  print(f\"{metric:<15} {value:0.5f}\")\n",
        "\n",
        "print(\"\\n Starting fine-tuning...\")\n",
        "t0 = time.time()\n",
        "trainer.train()\n",
        "t1 = time.time()\n",
        "print(\"Elapsed time: %.2fs\" % (t1-t0))\n",
        "\n",
        "print(\"\\n Evaluation after fine-tuning:\")\n",
        "eval_after_fine_tuning = trainer.evaluate()\n",
        "for metric, value in eval_after_fine_tuning.items():\n",
        "  print(f\"{metric:<15} {value:0.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdsWeuGGIVwI"
      },
      "source": [
        "Task 5 - Hyperparameter tuning and regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMQmW8LKIXXJ"
      },
      "outputs": [],
      "source": [
        "# Tracked using Wandb\n",
        "\n",
        "# Base Model: Overfitting (Training Loss is going down while Validation Loss is going up)\n",
        "# Run 1: Learning Rate Modification SMALLER -> (2e-5 -> 1e-5)\n",
        "# Run 2: Learning Rate Modification BIGGER -> (3e-5)\n",
        "# Run 3: Added Light Regularization -> (weight_decay=0.01)\n",
        "# Run 4: Regularization too strong -> (weight_decay=0.001)\n",
        "# Overfitting Fixed\n",
        "\n",
        "# Run 5: Added Learning Rate Schedule (No longer fixed) -> (warmup_ratio=0.1, lr_scheduler_type=\"linear\")\n",
        "# Run 6: Now Underfitting -> (warmup_ratio=0.02)\n",
        "# Run 7: Modify Learning Rate Type -> (lr_scheduler_type=\"constant_with_warmup\")\n",
        "# Run 8: Unstable, Adjust Learning Rate -> (3e-5 -> 2e-5)\n",
        "# Run 9: Revert, Increase Warmup Ratio -> (2e-5 -> 3e-5, warmup_ratio=0.05)\n",
        "# Learning Rate Schedule Not Helpful, Revert to Run 4\n",
        "\n",
        "# Run 10: Rerun of Run 4\n",
        "# Run 11: Slight Overfitting Still Exists -> (num_train_epochs=3)\n",
        "# Run 12: Revert Back to 4 Epochs, Add Early Stopping -> (num_train_epochs=4, + early stopping)\n",
        "# Run 13: Back Again to 3 Epochs -> (num_train_epochs=3)\n",
        "# Healthy Training Zone\n",
        "\n",
        "# Run 14: Rerun of 13 (i think) -> acc=80%, recall=0.88, precision=0.79 (lower) indicates model is catching most biased instances but makes more false positives\n",
        "# Attempt to stabilize high recall and lower precision (model curr flags too many as biased - a lot of false positives)\n",
        "\n",
        "# Run 15: lower learning rate (3e-5 -> 2e-5), added constant lr scheduler, stronger l2 regularization (weight decay 0.001 -> 0.01), gradient clipping (stability)\n",
        "\n",
        "\n",
        "# try lower learning rate (take smaller steps and adjust weights slower)\n",
        "# try adjusting batch size (8, 64, 128 -- test in powers of 2)\n",
        "# run 18: learning rate = 1.5e^-4 (decreasing validation loss which is good)\n",
        "# run 19: learning rate = 1.5e^-6 (validation loss seems to spike then fall)\n",
        "# run 20: learning rate = 4.5e^-4 (validation loss increases to 0.7 before leveling out)\n",
        "# run 21: learning rate = 3.5e^-4 (validation loss spikes and peaks before falling)\n",
        "# run 22: learning rate = 2.5e^-4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}